{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8023818,
          "sourceType": "datasetVersion",
          "datasetId": 4714673
        },
        {
          "sourceId": 25367,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 21376
        },
        {
          "sourceId": 25378,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 21385
        },
        {
          "sourceId": 25379,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 21386
        },
        {
          "sourceId": 25384,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 21391
        },
        {
          "sourceId": 25382,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 21389
        },
        {
          "sourceId": 25397,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 21402
        },
        {
          "sourceId": 25381,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 21388
        }
      ],
      "dockerImageVersionId": 30674,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import re\n",
        "import copy"
      ],
      "metadata": {
        "id": "LrucLlh3Hc3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a954a5af-e018-4936-b5df-13ead949b533",
        "execution": {
          "iopub.status.busy": "2024-04-04T07:00:44.432251Z",
          "iopub.execute_input": "2024-04-04T07:00:44.432902Z",
          "iopub.status.idle": "2024-04-04T07:01:02.470523Z",
          "shell.execute_reply.started": "2024-04-04T07:00:44.432856Z",
          "shell.execute_reply": "2024-04-04T07:01:02.469435Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.3.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "batch_size = 2\n",
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "epochs = 5\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "sample_every = 100\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# torch Dataset\n",
        "class GPT2Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, max_length=768):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>' + txt['prompt'] + txt['answer'], truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "# helper methods\n",
        "def train_valid_test(task_folder='multiArith/'):\n",
        "  '''split easy-cot data with 8:1:1 ratio'''\n",
        "  raw = load_dataset(\"json\", data_files=\"easy_cot/\" + task_folder + \"data.json\")\n",
        "  data = Dataset.from_pandas(pd.DataFrame(data=raw))\n",
        "  train_test_split = data.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
        "  train = train_test_split['train']\n",
        "  test = train_test_split['test']\n",
        "  valid_test_split = test.train_test_split(test_size=0.5, shuffle=False, seed=42)\n",
        "  valid = valid_test_split['train']\n",
        "  test = valid_test_split['test']\n",
        "  return train['train'], valid['train'], test['train']\n",
        "\n",
        "\n",
        "def save_model(model, tokenizer, output_dir):\n",
        "  if not os.path.exists(output_dir):\n",
        "      os.makedirs(output_dir)\n",
        "\n",
        "  print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model\n",
        "  model_to_save.save_pretrained(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "\n",
        "def load_model(output_dir):\n",
        "  model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "  model.to(device)\n",
        "  return model, tokenizer\n",
        "\n",
        "def extract_answer(output, is_generated=False):\n",
        "  if is_generated:\n",
        "    pattern = r'-->(.*)'\n",
        "  else:\n",
        "    pattern = r'-->(.*?)<'\n",
        "  m = re.search(pattern, output)\n",
        "  if m:\n",
        "      return m.group(1)\n",
        "  else:\n",
        "      return None\n",
        "\n",
        "def get_accuracy(model, tokenizer, test_set):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for sample in test_set:\n",
        "    prompt = sample['prompt']\n",
        "    answer = extract_answer(sample['answer'])\n",
        "    len_prompt = len(prompt)\n",
        "\n",
        "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "    generated = generated.to(device)\n",
        "\n",
        "    sample_outputs = model.generate(\n",
        "                                    generated,\n",
        "                                    #bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,\n",
        "                                    max_new_tokens=50,\n",
        "                                    num_return_sequences=1,\n",
        "                                    pad_token_id=tokenizer.eos_token_id\n",
        "                                    )\n",
        "\n",
        "    for i, sample_output in enumerate(sample_outputs):\n",
        "\n",
        "      # print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "      # print(tokenizer.decode(sample_output,skip_special_tokens=True)[len_prompt:])\n",
        "      extracted_answer = extract_answer(tokenizer.decode(sample_output, skip_special_tokens=True)[len_prompt:], is_generated=True)\n",
        "\n",
        "      # print(f'extracted: {extracted_answer}, truth: {answer}')\n",
        "      if extracted_answer is not None and extracted_answer.strip() == answer.strip():\n",
        "        correct += 1\n",
        "\n",
        "      total += 1\n",
        "\n",
        "  return correct/total\n",
        "\n",
        "\n",
        "def three_run(model, tokenizer, test_set):\n",
        "  avg_accuracy = 0\n",
        "  for _ in range(3):\n",
        "    avg_accuracy += get_accuracy(model, tokenizer, test_set)\n",
        "  avg_accuracy /= 3\n",
        "  print(f'Average accuracy over 3 runs: {avg_accuracy}')\n",
        "\n",
        "def run_test(model, tokenizer, valid, test):\n",
        "  # print(\"\")\n",
        "  # print(\"====== Experiment Results =======\")\n",
        "  # print(\"Running Default GPT2 on Task ...\")\n",
        "  # default_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration).to(device)\n",
        "  # default_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "  # print('validation--> ', end='')\n",
        "  # three_run(default_model, default_tokenizer, valid)\n",
        "  # print('test--> ', end='')\n",
        "  # three_run(default_model, default_tokenizer, test)\n",
        "  model.eval()\n",
        "  print(\"\")\n",
        "  print(\"======== Start of Results ========\")\n",
        "  if valid is not None:\n",
        "    print('validation--> ', end='')\n",
        "    three_run(model, tokenizer, valid)\n",
        "  print('test--> ', end='')\n",
        "  three_run(model, tokenizer, test)\n",
        "  print(\"======== End of Results ========\")"
      ],
      "metadata": {
        "id": "vrHQMc1fpfqu",
        "execution": {
          "iopub.status.busy": "2024-04-04T07:43:03.183704Z",
          "iopub.execute_input": "2024-04-04T07:43:03.184340Z",
          "iopub.status.idle": "2024-04-04T07:43:03.258807Z",
          "shell.execute_reply.started": "2024-04-04T07:43:03.184305Z",
          "shell.execute_reply": "2024-04-04T07:43:03.257857Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment"
      ],
      "metadata": {
        "id": "MH769Iw2w48J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_all(task_folder='multiArith/', model_dir='/kaggle/input/easycot_coinflip/other/hf/1/coinFlip'):\n",
        "    ''' Run Auto-CoT, Fine-tune-CoT, Easy-CoT on same task'''\n",
        "    default_configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "    default_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=default_configuration).to(device)\n",
        "    default_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "    model, tokenizer = load_model(model_dir)\n",
        "\n",
        "    raw = load_dataset(\"json\", data_files=\"/kaggle/input/easy-cot/easy_cot/\" + task_folder + \"data.json\", field=\"data\")\n",
        "    # test = context-augmented, test_cpy = original prompt\n",
        "    data = Dataset.from_pandas(pd.DataFrame(data=raw))\n",
        "    train_test_split = data.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
        "    test = train_test_split['test']['train']\n",
        "    test_cpy = copy.deepcopy(test)\n",
        "    # load demos\n",
        "    raw = load_dataset(\"json\", data_files=\"/kaggle/input/easy-cot/easy_cot/\" + task_folder + \"demos.json\", field=\"data\")\n",
        "    demos = raw['train']\n",
        "\n",
        "    context = \"\"\n",
        "    for sample in demos:\n",
        "        tmp_str = context + sample['demo'] + \"\\n\"\n",
        "        if len(tmp_str.split()) > 600:\n",
        "            break\n",
        "        context = tmp_str\n",
        "\n",
        "    for sample in test:\n",
        "        sample['prompt'] = context + sample['prompt']\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Fine-tune-CoT...\")\n",
        "    run_test(model, tokenizer, None, test_cpy)\n",
        "    print(\"\")\n",
        "    print(\"Running Auto-CoT...\")\n",
        "    run_test(default_model, default_tokenizer, None, test)\n",
        "    print(\"\")\n",
        "    print(\"Running Easy-CoT...\")\n",
        "    run_test(model, tokenizer, None, test)\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "id": "EohVO_NnSNoj",
        "execution": {
          "iopub.status.busy": "2024-04-04T07:59:51.975729Z",
          "iopub.execute_input": "2024-04-04T07:59:51.976450Z",
          "iopub.status.idle": "2024-04-04T07:59:51.987021Z",
          "shell.execute_reply.started": "2024-04-04T07:59:51.976415Z",
          "shell.execute_reply": "2024-04-04T07:59:51.985966Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "cl5nnDawOKuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_all('coinFlip/','/kaggle/input/easycot_coinflip/other/hf/1/coinFlip')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T08:08:40.812362Z",
          "iopub.execute_input": "2024-04-04T08:08:40.813048Z",
          "iopub.status.idle": "2024-04-04T08:11:21.690804Z",
          "shell.execute_reply.started": "2024-04-04T08:08:40.813012Z",
          "shell.execute_reply": "2024-04-04T08:11:21.689743Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "1bddda5449784b489519a388db8a7ba5",
            "146d156d552f498c970008ca80320841"
          ]
        },
        "id": "lhvrJPQNuOWI",
        "outputId": "ee3ccc6d-0a03-42bc-9421-49cda04d065f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bddda5449784b489519a388db8a7ba5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "146d156d552f498c970008ca80320841"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nRunning Fine-tune-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.020202020202020204\n======== End of Results ========\n\nRunning Auto-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.04040404040404041\n======== End of Results ========\n\nRunning Easy-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.5151515151515151\n======== End of Results ========\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_all('lastLetter/','/kaggle/input/easycot_letter/other/hf/1/lastLetter')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T10:32:43.625317Z",
          "iopub.execute_input": "2024-04-04T10:32:43.625703Z",
          "iopub.status.idle": "2024-04-04T10:35:29.525559Z",
          "shell.execute_reply.started": "2024-04-04T10:32:43.625668Z",
          "shell.execute_reply": "2024-04-04T10:35:29.524631Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "cf01bb17e88c4e7197db2fa05a5e69b2",
            "52134069e28348dbb089c350090fc353"
          ]
        },
        "id": "VgLVNpKEuOWJ",
        "outputId": "39693fb6-7e18-4233-8cbe-fedc5e46ed97"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf01bb17e88c4e7197db2fa05a5e69b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52134069e28348dbb089c350090fc353"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nRunning Fine-tune-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.0\n======== End of Results ========\n\nRunning Auto-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.0\n======== End of Results ========\n\nRunning Easy-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.0\n======== End of Results ========\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_all('commonSenseQA/','/kaggle/input/easycot_csqa/other/hf/1/commonSenseQA')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T08:31:47.902160Z",
          "iopub.execute_input": "2024-04-04T08:31:47.902917Z",
          "iopub.status.idle": "2024-04-04T09:25:58.644526Z",
          "shell.execute_reply.started": "2024-04-04T08:31:47.902885Z",
          "shell.execute_reply": "2024-04-04T09:25:58.643514Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "d26c53d1b28e49eda6b7193faa81141d",
            "5e114f25a69e4b7885327dc8a0d3822c"
          ]
        },
        "id": "YqV3DdrquOWJ",
        "outputId": "f28c2677-a333-4e17-9173-98b3c371cad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d26c53d1b28e49eda6b7193faa81141d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e114f25a69e4b7885327dc8a0d3822c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nRunning Fine-tune-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.04655408489274304\n======== End of Results ========\n\nRunning Auto-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.018256503879507075\n======== End of Results ========\n\nRunning Easy-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.18575992697398447\n======== End of Results ========\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_all('strategyQA/','/kaggle/input/easycot_sqa/other/hf/1/strategyQA')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T08:17:10.938273Z",
          "iopub.execute_input": "2024-04-04T08:17:10.938957Z",
          "iopub.status.idle": "2024-04-04T08:31:15.076726Z",
          "shell.execute_reply.started": "2024-04-04T08:17:10.938910Z",
          "shell.execute_reply": "2024-04-04T08:31:15.075716Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "36ec7ffa55464e92b57daab46e6074ad",
            "9b7ef862d9114bf09750cfd6356c2cdc"
          ]
        },
        "id": "Z_dRzgcOuOWJ",
        "outputId": "9b5e0099-5a53-4d7c-e364-5b7ac9dea8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36ec7ffa55464e92b57daab46e6074ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b7ef862d9114bf09750cfd6356c2cdc"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nRunning Fine-tune-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.06345733041575492\n======== End of Results ========\n\nRunning Auto-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.024070021881838075\n======== End of Results ========\n\nRunning Easy-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.0\n======== End of Results ========\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_all('multiArith/','/kaggle/input/easycot_ma/other/hf/1/multiArith')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T07:59:58.065385Z",
          "iopub.execute_input": "2024-04-04T07:59:58.065752Z",
          "iopub.status.idle": "2024-04-04T08:03:07.609994Z",
          "shell.execute_reply.started": "2024-04-04T07:59:58.065725Z",
          "shell.execute_reply": "2024-04-04T08:03:07.608974Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "c82ea1f7281546b2b5efcfd2f7fc64ff",
            "dd35c05aca8942448587b8e9a497e1c4"
          ]
        },
        "id": "mRQ55-PjuOWK",
        "outputId": "602f77ff-dae6-4c18-8532-4cacbf476527"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c82ea1f7281546b2b5efcfd2f7fc64ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd35c05aca8942448587b8e9a497e1c4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nRunning Fine-tune-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.008403361344537815\n======== End of Results ========\n\nRunning Auto-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.01680672268907563\n======== End of Results ========\n\nRunning Easy-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.025210084033613446\n======== End of Results ========\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_all('gsm8k/','/kaggle/input/easycot_gsm8k/other/hf/1/gsm8k')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-04T09:25:58.646283Z",
          "iopub.execute_input": "2024-04-04T09:25:58.646546Z",
          "iopub.status.idle": "2024-04-04T10:17:23.111815Z",
          "shell.execute_reply.started": "2024-04-04T09:25:58.646523Z",
          "shell.execute_reply": "2024-04-04T10:17:23.110750Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "e883c32e949c4810bb2e7f5b6815f410",
            "ebd3b5a8f08d4e19938bf85c4f0e333b"
          ]
        },
        "id": "yjJNrDY5uOWK",
        "outputId": "60434c87-4152-41a9-84d0-99c32c577753"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e883c32e949c4810bb2e7f5b6815f410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebd3b5a8f08d4e19938bf85c4f0e333b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\nRunning Fine-tune-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.0028457598178713715\n======== End of Results ========\n\nRunning Auto-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.0011383039271485487\n======== End of Results ========\n\nRunning Easy-CoT...\n\n======== Start of Results ========\ntest--> Average accuracy over 3 runs: 0.008537279453614115\n======== End of Results ========\n\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}
